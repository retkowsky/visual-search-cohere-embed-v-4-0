{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Search - Web Application\n",
    "\n",
    "<img src=\"https://github.com/retkowsky/images/blob/master/cohere4.jpg?raw=true\" width=400>\n",
    "This notebook demonstrates how to build and deploy an interactive web application for multimodal visual search using Gradio. \n",
    "\n",
    "It combines Azure AI services (embeddings, search) with an intuitive user interface that supports both text-based and image-based search across a fashion catalog.\n",
    "\n",
    "The application provides an end-to-end solution for deploying semantic search capabilities as a shareable web interface, enabling users to discover fashion items using natural language descriptions or reference images without requiring technical knowledge.\n",
    "\n",
    "The notebook demonstrates five core capabilities:\n",
    "- Web UI Framework - Gradio-based responsive interface with theme customization\n",
    "- Text-to-Image Search - Find fashion items using natural language descriptions\n",
    "- Image-to-Image Search - Discover similar items by uploading a reference image\n",
    "- Configurable Results - Adjustable number of results (1-20) via slider controls\n",
    "- Results Gallery - Grid-based image display with similarity scores and download functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "import sys\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "from azure.ai.inference import EmbeddingsClient, ImageEmbeddingsClient\n",
    "from azure.ai.inference.models import ImageEmbeddingInput\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from azure.search.documents.models import VectorizedQuery\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from PIL import Image\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.10.18 (main, Jun  5 2025, 13:14:17) [GCC 11.2.0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Python version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today is 11-Dec-2025 14:03:04\n"
     ]
    }
   ],
   "source": [
    "print(f\"Today is {datetime.today().strftime('%d-%b-%Y %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\"azure.env\")\n",
    "\n",
    "# Cohere v4\n",
    "api_key = os.getenv(\"api_key\")\n",
    "endpoint = os.getenv(\"endpoint\")\n",
    "\n",
    "# Azure AI Search\n",
    "azure_search_endpoint = os.getenv(\"azure_search_endpoint\")\n",
    "azure_search_key = os.getenv(\"azure_search_key\")\n",
    "\n",
    "# Azure storage account\n",
    "blob_connection_string = os.getenv(\"blob_connection_string\")\n",
    "container_name = os.getenv(\"container_name\")\n",
    "\n",
    "deployment_name = \"embed-v-4-0\"  # name of Cohere Embed 4 model as deployed in Microsoft Foundry\n",
    "index_name = \"fashion-demo-2025\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_client = EmbeddingsClient(endpoint=endpoint,\n",
    "                               credential=AzureKeyCredential(api_key),\n",
    "                               model=deployment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_client = ImageEmbeddingsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(api_key),\n",
    "    model=deployment_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_embeddings(text: str) -> list[float]:\n",
    "    \"\"\"\n",
    "    Generate text embeddings using Cohere V4 embedding model.\n",
    "    \n",
    "    Converts input text into a dense vector representation (embedding) that\n",
    "    captures semantic meaning, suitable for similarity comparisons, clustering,\n",
    "    or retrieval tasks. Uses the configured deployment and text client for\n",
    "    Azure OpenAI API access.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text to embed. Must not be empty or whitespace-only.\n",
    "    \n",
    "    Returns:\n",
    "        list[float]: A list of floats representing the embedding vector for the input text.\n",
    "    \"\"\"\n",
    "    if not text or not text.strip():\n",
    "        raise ValueError(\"Input text cannot be empty\")\n",
    "\n",
    "    try:\n",
    "        response = text_client.embed(\n",
    "            input=[text.strip()],\n",
    "            model=deployment_name,\n",
    "        )\n",
    "        return response.data[0].embedding\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Error\")\n",
    "        logging.error(f\"Failed to generate embeddings: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_local_image_embeddings(image_path: str) -> list[float]:\n",
    "    \"\"\"\n",
    "    Generate embeddings for a local image file using Cohere Embed 4.\n",
    "    \n",
    "    Converts an image file into a dense vector representation (embedding) that\n",
    "    captures visual content and semantic meaning. The embedding can be used for\n",
    "    image similarity comparisons, visual search, or multimodal retrieval tasks.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): The file path to a local JPG image file to embed.\n",
    "    \n",
    "    Returns:\n",
    "        list[float]: A list of floats representing the embedding vector for the input image.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        image_input = ImageEmbeddingInput.load(\n",
    "            image_file=image_path,\n",
    "            image_format=\"jpg\",\n",
    "        )\n",
    "        response = image_client.embed(input=[image_input])\n",
    "        return response.data[0].embedding\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Error\")\n",
    "        logging.error(f\"Failed to generate embeddings: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_embeddings_from_blob(blob_name: str) -> list[float]:\n",
    "    \"\"\"\n",
    "    Generate embeddings with Cohere Embed 4 for an image stored in Azure Blob Storage.\n",
    "    \n",
    "    Downloads an image blob to a temporary local file, generates embeddings using\n",
    "    Azure OpenAI's vision model, and cleans up the temporary file. This function\n",
    "    bridges blob storage access with the embedding generation pipeline for images\n",
    "    hosted in cloud storage.\n",
    "    \n",
    "    Args:\n",
    "        blob_name (str): The name/path of the image blob in the configured container.\n",
    "    \n",
    "    Returns:\n",
    "        list[float]: A list of floats representing the embedding vector for the blob image.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Download blob to temporary file\n",
    "        blob_service_client = BlobServiceClient.from_connection_string(\n",
    "            blob_connection_string)\n",
    "        blob_client = blob_service_client.get_blob_client(\n",
    "            container=container_name, blob=blob_name)\n",
    "\n",
    "        # Create temp file with proper extension\n",
    "        with tempfile.NamedTemporaryFile(delete=False,\n",
    "                                         suffix='.jpg') as tmp_file:\n",
    "            tmp_path = tmp_file.name\n",
    "            blob_data = blob_client.download_blob()\n",
    "            blob_data.readinto(tmp_file)\n",
    "\n",
    "        # Generate embeddings using your existing function\n",
    "        embeddings = get_local_image_embeddings(tmp_path)\n",
    "\n",
    "        # Clean up temp file\n",
    "        os.unlink(tmp_path)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Error\")\n",
    "        logging.error(f\"Failed to process blob image: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_stats(index_name: str) -> tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Retrieve statistics for an Azure AI Search index.\n",
    "    \n",
    "    Fetches index metadata including document count and storage size from the\n",
    "    Azure AI Search service. Uses the REST API directly to query index statistics,\n",
    "    providing insights into index usage and capacity.\n",
    "    \n",
    "    Args:\n",
    "        index_name (str): The name of the search index to retrieve statistics for.\n",
    "    \n",
    "    Returns:\n",
    "        tuple[int, int]: A tuple containing:\n",
    "            - document_count (int): The total number of documents in the index\n",
    "            - storage_size (int): The total storage size in bytes used by the index\n",
    "    \n",
    "    Raises:\n",
    "        requests.exceptions.RequestException: If the API request fails\n",
    "        KeyError: If expected fields are missing from the response\n",
    "    \"\"\"\n",
    "    url = f\"{azure_search_endpoint}/indexes/{index_name}/stats?api-version=2025-09-01\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"api-key\": azure_search_key,\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        stats = response.json()\n",
    "        print(f\"‚úÖ Azure AI Search index statistics for: {index_name}\\n\")\n",
    "        print(json.dumps(stats, indent=2))\n",
    "        \n",
    "        document_count = stats['documentCount']\n",
    "        storage_size = stats['storageSize']\n",
    "        \n",
    "        return document_count, storage_size\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"‚ùå Request error: {e}\")\n",
    "        raise\n",
    "    except KeyError as e:\n",
    "        print(f\"‚ùå Missing expected field in response: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Unexpected error: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_search_gradio(prompt: str, topn: int = 5) -> List:\n",
    "    \"\"\"\n",
    "    Prompt search for the gradio webapp\n",
    "    \"\"\"\n",
    "    results_list = []\n",
    "    images_list = []\n",
    "    imgsize = 360\n",
    "\n",
    "    # Initialize the Azure AI Search client\n",
    "    search_client = SearchClient(azure_search_endpoint, index_name,\n",
    "                                 AzureKeyCredential(azure_search_key))\n",
    "\n",
    "    # Perform vector search\n",
    "    query_vector = get_text_embeddings(prompt)\n",
    "\n",
    "    request = search_client.search(None,\n",
    "                                   vector_queries=[\n",
    "                                       VectorizedQuery(\n",
    "                                           vector=query_vector,\n",
    "                                           k_nearest_neighbors=topn,\n",
    "                                           fields=\"imagevector\")\n",
    "                                   ])\n",
    "\n",
    "    results = [(doc[\"imagefile\"], doc[\"@search.score\"]) for doc in request]\n",
    "\n",
    "    for result in results:\n",
    "        image_file = result[0]\n",
    "        results_list.append(image_file)\n",
    "\n",
    "    for image_file in results_list:\n",
    "        blob_client = container_client.get_blob_client(image_file)\n",
    "        blob_image = blob_client.download_blob().readall()\n",
    "        img = Image.open(io.BytesIO(blob_image)).resize((imgsize, imgsize))\n",
    "        images_list.append(img)\n",
    "\n",
    "    return images_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_search_gradio(imagefile: str, topn: int = 5) -> List:\n",
    "    \"\"\"\n",
    "     Performs an image-based search for the Gradio web app and returns the top N image results.\n",
    "\n",
    "    Args:\n",
    "        imagefile (str): The image file to be vectorized and used for the search.\n",
    "        topn (int, optional): The number of top results to return. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of PIL Image objects representing the top N search results.\n",
    "    \"\"\"\n",
    "    results_list = []\n",
    "    images_list = []\n",
    "    imgsize = 360\n",
    "\n",
    "    # Azure AI search client\n",
    "    search_client = SearchClient(azure_search_endpoint, index_name,\n",
    "                                 AzureKeyCredential(azure_search_key))\n",
    "\n",
    "    query_vector = get_local_image_embedding(imagefile)\n",
    "    request = search_client.search(None,\n",
    "                                   vector_queries=[\n",
    "                                       VectorizedQuery(\n",
    "                                           vector=query_vector,\n",
    "                                           k_nearest_neighbors=topn,\n",
    "                                           fields=\"imagevector\")\n",
    "                                   ])\n",
    "\n",
    "    # Assuming the search results include cosine similarity scores\n",
    "    results = [(doc[\"imagefile\"], doc[\"@search.score\"]) for doc in request]\n",
    "\n",
    "    print(\"\\033[1;34m\", end=\"\")\n",
    "    for idx, (filename, score) in enumerate(results, start=1):\n",
    "        print(f\"Top {idx:02}: {filename} with Cosine Similarity = {score:.5}\")\n",
    "        images_list.append(filename)\n",
    "\n",
    "    for image_file in images_list:\n",
    "        blob_client = container_client.get_blob_client(image_file)\n",
    "        blob_image = blob_client.download_blob().readall()\n",
    "        img = Image.open(io.BytesIO(blob_image)).resize((imgsize, imgsize))\n",
    "        results_list.append(img)\n",
    "\n",
    "    return results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_index(index_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Deletes an Azure AI Search index.\n",
    "\n",
    "    Args:\n",
    "        index_name (str): The name of the index to be deleted.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    search_client = SearchIndexClient(\n",
    "        endpoint=azure_search_endpoint,\n",
    "        credential=AzureKeyCredential(azure_search_key))\n",
    "\n",
    "    print(f\"üóëÔ∏è Deleting the Azure AI Search index: {index_name}\")\n",
    "    search_client.delete_index(index_name)\n",
    "    print(\"‚úÖ Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blob storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL of the first blob: https://azurestorageaccountsr.blob.core.windows.net/fashionimages/fashion/0390469004.jpg\n"
     ]
    }
   ],
   "source": [
    "# Connect to Blob Storage\n",
    "blob_service_client = BlobServiceClient.from_connection_string(\n",
    "    blob_connection_string)\n",
    "container_client = blob_service_client.get_container_client(container_name)\n",
    "blobs = container_client.list_blobs()\n",
    "\n",
    "first_blob = next(blobs)\n",
    "blob_url = container_client.get_blob_client(first_blob).url\n",
    "print(f\"URL of the first blob: {blob_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure AI Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the Azure AI Search client\n",
      "‚úÖ Done\n",
      "<azure.search.documents.indexes._search_index_client.SearchIndexClient object at 0x7428c09aee30>\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Setting the Azure AI Search client\n",
    "    print(\"Setting the Azure AI Search client\")\n",
    "    search_client = SearchIndexClient(\n",
    "        endpoint=azure_search_endpoint,\n",
    "        credential=AzureKeyCredential(azure_search_key))\n",
    "    print(\"‚úÖ Done\")\n",
    "    print(search_client)\n",
    "\n",
    "except:\n",
    "    print(\n",
    "        f\"‚ùå Request failed. Cannot create Azure AI Search client: {azure_search_endpoint}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Azure AI Search index statistics for: fashion-demo-2025\n",
      "\n",
      "{\n",
      "  \"@odata.context\": \"https://azureaisearch-sr.search.windows.net/$metadata#Microsoft.Azure.Search.V2025_09_01.IndexStatistics\",\n",
      "  \"documentCount\": 2000,\n",
      "  \"storageSize\": 31724997,\n",
      "  \"vectorIndexSize\": 12402620\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "document_count, storage_size = get_index_stats(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Number of documents in the index = 2,000\n",
      "üíæ Size of the index = 30.26 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"üìä Number of documents in the index = {document_count:,}\")\n",
    "print(f\"üíæ Size of the index = {storage_size / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_search_gradio(prompt: str, topn: int = 10) -> List:\n",
    "    \"\"\"\n",
    "    Prompt search for the gradio webapp\n",
    "    \"\"\"\n",
    "    results_list = []\n",
    "    images_list = []\n",
    "    imgsize = 360\n",
    "    # Initialize the Azure AI Search client\n",
    "    search_client = SearchClient(azure_search_endpoint, index_name,\n",
    "                                 AzureKeyCredential(azure_search_key))\n",
    "\n",
    "    # Perform vector search\n",
    "    query_vector = get_text_embeddings(prompt)\n",
    "    request = search_client.search(None,\n",
    "                                   vector_queries=[\n",
    "                                       VectorizedQuery(\n",
    "                                           vector=query_vector,\n",
    "                                           k_nearest_neighbors=topn,\n",
    "                                           fields=\"imagevector\")\n",
    "                                   ])\n",
    "    results = [(doc[\"imagefile\"], doc[\"@search.score\"]) for doc in request]\n",
    "\n",
    "    # Build gallery with images and captions\n",
    "    for image_file, score in results:\n",
    "        blob_client = container_client.get_blob_client(image_file)\n",
    "        blob_image = blob_client.download_blob().readall()\n",
    "        img = Image.open(io.BytesIO(blob_image)).resize((imgsize, imgsize))\n",
    "        images_list.append((img, f\"{os.path.basename(image_file)}\\nscore = {score:.4f}\"))\n",
    "\n",
    "    return images_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_search_gradio(imagefile: str, topn: int = 10) -> List:\n",
    "    \"\"\"\n",
    "     Performs an image-based search for the Gradio web app and returns the top N image results.\n",
    "    Args:\n",
    "        imagefile (str): The image file to be vectorized and used for the search.\n",
    "        topn (int, optional): The number of top results to return. Defaults to 5.\n",
    "    Returns:\n",
    "        list: A list of tuples (PIL Image, caption) representing the top N search results.\n",
    "    \"\"\"\n",
    "    results_list = []\n",
    "    images_list = []\n",
    "    imgsize = 360\n",
    "    # Azure AI search client\n",
    "    search_client = SearchClient(azure_search_endpoint, index_name,\n",
    "                                 AzureKeyCredential(azure_search_key))\n",
    "    query_vector = get_local_image_embeddings(imagefile)\n",
    "    request = search_client.search(None,\n",
    "                                   vector_queries=[\n",
    "                                       VectorizedQuery(\n",
    "                                           vector=query_vector,\n",
    "                                           k_nearest_neighbors=topn,\n",
    "                                           fields=\"imagevector\")\n",
    "                                   ])\n",
    "    # Assuming the search results include cosine similarity scores\n",
    "    results = [(doc[\"imagefile\"], doc[\"@search.score\"]) for doc in request]\n",
    "    print(\"\\033[1;34m\", end=\"\")\n",
    "    for idx, (filename, score) in enumerate(results, start=1):\n",
    "        print(f\"Top {idx:02}: {filename} with Cosine Similarity = {score:.4}\")\n",
    "\n",
    "    # Build gallery with images and captions\n",
    "    for image_file, score in results:\n",
    "        blob_client = container_client.get_blob_client(image_file)\n",
    "        blob_image = blob_client.download_blob().readall()\n",
    "        img = Image.open(io.BytesIO(blob_image)).resize((imgsize, imgsize))\n",
    "        results_list.append((img, f\"{os.path.basename(image_file)}\\nScore = {score:.4f}\"))\n",
    "\n",
    "    return results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unified_search(text_query: str, \n",
    "                   image_input, \n",
    "                   search_type: str, \n",
    "                   top_n: int):\n",
    "    \"\"\"\n",
    "    Unified search function that handles both text and image searches.\n",
    "    Returns a tuple of (gallery_results, status_message)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if search_type == \"Text\":\n",
    "            if not text_query or text_query.strip() == \"\":\n",
    "                return [], \"‚ùå Please enter a text query\"\n",
    "            start = time.time()\n",
    "            results = prompt_search_gradio(text_query, top_n)\n",
    "            elapsed = time.time() - start\n",
    "            # Results should be a list of tuples (image, caption) for the gallery\n",
    "            if isinstance(results, (list, tuple)):\n",
    "                return results, f\"‚úÖ Text search completed in {elapsed:.2f} seconds\"\n",
    "            return results, f\"‚úÖ Text search completed in {elapsed:.2f} seconds\"\n",
    "        \n",
    "        elif search_type == \"Image\":\n",
    "            if image_input is None:\n",
    "                return [], \"‚ùå Please upload an image\"\n",
    "            start = time.time()\n",
    "            results = image_search_gradio(image_input, top_n)\n",
    "            elapsed = time.time() - start\n",
    "            # Results should be a list of tuples (image, caption) for the gallery\n",
    "            if isinstance(results, (list, tuple)):\n",
    "                return results, f\"‚úÖ Image search completed in {elapsed:.2f} seconds\"\n",
    "            return results, f\"‚úÖ Image search completed in {elapsed:.2f} seconds\"\n",
    "        \n",
    "        else:\n",
    "            return [], \"‚ùå Please select a search type\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return [], f\"‚ùå Error during search: {str(e)}\"\n",
    "\n",
    "with gr.Blocks() as webapp:\n",
    "    \n",
    "    # Header\n",
    "    gr.HTML(\"\"\"\n",
    "        <div id=\"header\">\n",
    "            <h1>üîç AI-Powered Visual Search using Cohere Embed 4</h1>\n",
    "            <p>Powered by Microsoft Foundry</p>\n",
    "        </div>\n",
    "    \"\"\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            with gr.Group(elem_classes=\"search-container\"):\n",
    "                gr.Markdown(\"## üéØ Search Setup\")\n",
    "                \n",
    "                # Search Type Selection\n",
    "                with gr.Group():\n",
    "                    gr.Markdown(\"### 1Ô∏è‚É£ Select search type\")\n",
    "                    search_type = gr.Radio(\n",
    "                        choices=[\"Text\", \"Image\"],\n",
    "                        value=\"Text\",\n",
    "                        label=\"Search by:\",\n",
    "                        interactive=True\n",
    "                    )\n",
    "                \n",
    "                # Text Input Section\n",
    "                with gr.Group(visible=True) as text_group:\n",
    "                    gr.Markdown(\"### 2Ô∏è‚É£ Enter text description\")\n",
    "                    text_input = gr.Textbox(\n",
    "                        label=\"What are you looking for?\",\n",
    "                        placeholder=\"e.g., a red dress, blue shirt...\",\n",
    "                        lines=1,\n",
    "                        interactive=True\n",
    "                    )\n",
    "                \n",
    "                # Image Upload Section\n",
    "                with gr.Group(visible=False) as image_group:\n",
    "                    gr.Markdown(\"### 2Ô∏è‚É£ Upload reference image\")\n",
    "                    image_input = gr.Image(\n",
    "                        label=\"Upload Image\",\n",
    "                        type=\"filepath\",\n",
    "                        interactive=True\n",
    "                    )\n",
    "                    gr.Markdown(\"*Upload an image to find visually similar results*\")\n",
    "                \n",
    "                # Number of Results\n",
    "                with gr.Group():\n",
    "                    gr.Markdown(\"### 3Ô∏è‚É£ Number of images\")\n",
    "                    top_n = gr.Slider(\n",
    "                        minimum=1,\n",
    "                        maximum=20,\n",
    "                        value=8,\n",
    "                        step=1,\n",
    "                        label=\"Number of results to display\"\n",
    "                    )\n",
    "                \n",
    "                # Search Button\n",
    "                search_btn = gr.Button(\n",
    "                    \"SEARCH IMAGES\",\n",
    "                    variant=\"primary\",\n",
    "                    size=\"lg\",\n",
    "                    elem_id=\"search-button\",\n",
    "                    scale=2\n",
    "                )\n",
    "                \n",
    "                # Status Message\n",
    "                status_msg = gr.Markdown(\n",
    "                    value=\"\",\n",
    "                    elem_classes=\"status-message\"\n",
    "                )\n",
    "    \n",
    "    # Results Section\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            with gr.Group(elem_classes=\"gallery-container\"):\n",
    "                gr.Markdown(\"## üéØ Visual search results\")\n",
    "                results_gallery = gr.Gallery(\n",
    "                    label=\"\",\n",
    "                    show_label=False,\n",
    "                    columns=4,\n",
    "                    rows=5,\n",
    "                    object_fit=\"cover\"\n",
    "                )\n",
    "    \n",
    "    # Footer\n",
    "    gr.HTML(\"\"\"\n",
    "        <footer>\n",
    "            <p>Powered by Microsoft Foundry | Built with Gradio</p>\n",
    "        </footer>\n",
    "    \"\"\")\n",
    "    \n",
    "    # Toggle visibility based on search type\n",
    "    def toggle_search_input(search_choice):\n",
    "        return gr.Group(visible=(search_choice == \"Text\")), gr.Group(visible=(search_choice == \"Image\"))\n",
    "    \n",
    "    search_type.change(\n",
    "        fn=toggle_search_input,\n",
    "        inputs=search_type,\n",
    "        outputs=[text_group, image_group]\n",
    "    )\n",
    "    \n",
    "    # Event handler for unified search\n",
    "    search_btn.click(\n",
    "        fn=unified_search,\n",
    "        inputs=[text_input, image_input, search_type, top_n],\n",
    "        outputs=[results_gallery, status_msg]\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://e343a345797547c93a.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://e343a345797547c93a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webapp.launch(share=True, show_error=True, theme=gr.themes.Soft())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post processing\n",
    "\n",
    "We can delete the index if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Azure AI Search index statistics for: fashion-demo-2025\n",
      "\n",
      "{\n",
      "  \"@odata.context\": \"https://azureaisearch-sr.search.windows.net/$metadata#Microsoft.Azure.Search.V2025_09_01.IndexStatistics\",\n",
      "  \"documentCount\": 2000,\n",
      "  \"storageSize\": 31724997,\n",
      "  \"vectorIndexSize\": 12402620\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2000, 31724997)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_index_stats(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete_index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 - SDK v2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
